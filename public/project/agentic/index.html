<!doctype html>
<!-- This site was created with Hugo Blox. https://hugoblox.com -->
<!-- Last Published: March 10, 2025 --><html lang="en-us" dir="ltr"
      data-wc-theme-default="system">
  
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="generator" content="Hugo Blox Builder 0.3.1" />

  
  












  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Armstrong Foundjem" />

  
  
  
    
  
  <meta name="description" content="Agentic AI Systems in Multi-Environment Settings 1. Introduction to Agentic AI An Agentic AI System refers to an autonomous AI system that can sense, decide, and act in an environment to achieve specific goals. In multi-environment settings, these AI agents operate across diverse, dynamic, and often conflicting environments, requiring adaptive decision-making, communication, and coordination.
" />

  
  <link rel="alternate" hreflang="en-us" href="https://foundjem.github.io/project/agentic/" />

  
  
  
  
    
    <link rel="stylesheet" href="/css/themes/emerald.min.css" />
  

  
  
    
    <link href="/dist/wc.min.css" rel="stylesheet" />
  

  
  
  

  

  <script>
     
    window.hbb = {
       defaultTheme: document.documentElement.dataset.wcThemeDefault,
       setDarkTheme: () => {
        document.documentElement.classList.add("dark");
        document.documentElement.style.colorScheme = "dark";
      },
       setLightTheme: () => {
        document.documentElement.classList.remove("dark");
        document.documentElement.style.colorScheme = "light";
      }
    }

    console.debug(`Default Hugo Blox Builder theme is ${window.hbb.defaultTheme}`);

    if ("wc-color-theme" in localStorage) {
      localStorage.getItem("wc-color-theme") === "dark" ? window.hbb.setDarkTheme() : window.hbb.setLightTheme();
    } else {
      window.hbb.defaultTheme === "dark" ? window.hbb.setDarkTheme() : window.hbb.setLightTheme();
      if (window.hbb.defaultTheme === "system") {
        window.matchMedia("(prefers-color-scheme: dark)").matches ? window.hbb.setDarkTheme() : window.hbb.setLightTheme();
      }
    }
  </script>

  <script>
    
    document.addEventListener('DOMContentLoaded', function () {
      
      let checkboxes = document.querySelectorAll('li input[type=\'checkbox\'][disabled]');
      checkboxes.forEach(e => {
        e.parentElement.parentElement.classList.add('task-list');
      });

      
      const liNodes = document.querySelectorAll('.task-list li');
      liNodes.forEach(nodes => {
        let textNodes = Array.from(nodes.childNodes).filter(node => node.nodeType === 3 && node.textContent.trim().length > 1);
        if (textNodes.length > 0) {
          const span = document.createElement('label');
          textNodes[0].after(span);  
          span.appendChild(nodes.querySelector('input[type=\'checkbox\']'));
          span.appendChild(textNodes[0]);
        }
      });
    });
  </script>

  
  
  




































  
  

  
  <link rel="icon" type="image/png" href="/media/icon_hu_5cac115d14ed3d49.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hu_e7f327b39c12e7ca.png" />

  <link rel="canonical" href="https://foundjem.github.io/project/agentic/" />

  
  
  
  
  
  
  
  
    
  
  

  
  
    
    
  
  <meta property="twitter:card" content="summary_large_image" />
  
    <meta property="twitter:site" content="@GetResearchDev" />
    <meta property="twitter:creator" content="@GetResearchDev" />
  
  <meta property="og:site_name" content="Armstrong website" />
  <meta property="og:url" content="https://foundjem.github.io/project/agentic/" />
  <meta property="og:title" content="Multi-Agent AI Systems | Armstrong website" />
  <meta property="og:description" content="Agentic AI Systems in Multi-Environment Settings
1. Introduction to Agentic AI
An Agentic AI System refers to an autonomous AI system that can sense, decide, and act in an environment to achieve specific goals. In multi-environment settings, these AI agents operate across diverse, dynamic, and often conflicting environments, requiring adaptive decision-making, communication, and coordination." /><meta property="og:image" content="https://foundjem.github.io/project/agentic/featured.png" />
    <meta property="twitter:image" content="https://foundjem.github.io/project/agentic/featured.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta
        property="article:published_time"
        content="2025-02-11T00:00:00&#43;00:00"
      />
    
    <meta property="article:modified_time" content="2025-02-15T11:49:13-05:00">
  

  



  


  <title>Multi-Agent AI Systems | Armstrong website</title>

  
  
  
  
  
    
    
  
  
  <style>
    @font-face {
      font-family: 'Inter var';
      font-style: normal;
      font-weight: 100 900;
      font-display: swap;
      src: url(/dist/font/Inter.var.woff2) format(woff2);
    }
  </style>

  

  
  


  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
  
  
  
  
  
  
  
  
  
  
  
  
  
  
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
    
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
  







<link type="text/css" rel="stylesheet" href="/dist/pagefind/pagefind-ui.be766eb419317a14ec769d216e9779bfe8f3737c80e780f4ba0dafb57a41a482.css" integrity="sha256-vnZutBkxehTsdp0hbpd5v&#43;jzc3yA54D0ug2vtXpBpII=" />


<script src="/dist/pagefind/pagefind-ui.87693d7c6f2b3b347ce359d0ede762c033419f0a32b22ce508c335a81d841f1b.js" integrity="sha256-h2k9fG8rOzR841nQ7ediwDNBnwoysizlCMM1qB2EHxs="></script>


<script>window.hbb.pagefind = {"baseUrl":"/"};</script>

<style>
  html.dark {
    --pagefind-ui-primary: #eeeeee;
    --pagefind-ui-text: #eeeeee;
    --pagefind-ui-background: #152028;
    --pagefind-ui-border: #152028;
    --pagefind-ui-tag: #152028;
  }
</style>

<script>
  window.addEventListener('DOMContentLoaded', (event) => {
    new PagefindUI({
      element: "#search",
      showSubResults: true,
      baseUrl: window.hbb.pagefind.baseUrl,
      bundlePath: window.hbb.pagefind.baseUrl + "pagefind/",
    });
  });
  document.addEventListener('DOMContentLoaded', () => {
    let element = document.getElementById('search');
    let trigger = document.getElementById('search_toggle');

    if (trigger) {
      trigger.addEventListener('click', () => {
        element.classList.toggle('hidden');
        element.querySelector("input").value = ""
        element.querySelector("input").focus()

        if (!element.classList.contains('hidden')) {
          let clear_trigger = document.querySelector('.pagefind-ui__search-clear');

          if (clear_trigger && !clear_trigger.hasAttribute('listenerOnClick')) {
            clear_trigger.setAttribute('listenerOnClick', 'true');

            clear_trigger.addEventListener('click', () => {
              element.classList.toggle('hidden');
            });
          }
        }

      });
    }
  });
</script>










  
  
  <link type="text/css" rel="stylesheet" href="/dist/lib/katex/katex.min.505d5f829022bb7b4f24dfee0aa1141cd7bba67afe411d1240335f820960b5c3.css" integrity="sha256-UF1fgpAiu3tPJN/uCqEUHNe7pnr&#43;QR0SQDNfgglgtcM=" />
  
  
  <script defer src="/dist/lib/katex/katex.min.dc84b296ec3e884de093158f760fd9d45b6c7abe58b5381557f4e138f46a58ae.js" integrity="sha256-3ISyluw&#43;iE3gkxWPdg/Z1Ftser5YtTgVV/ThOPRqWK4="></script>
  
  
  
  
  <script defer src="/js/katex-renderer.6579ec9683211cfb952064aedf3a3baea5eeb17a061775b32b70917474637c80.js" integrity="sha256-ZXnsloMhHPuVIGSu3zo7rqXusXoGF3WzK3CRdHRjfIA="></script>
  
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  






  
  
  
  
  
  
  
  <script
    defer
    src="/js/hugo-blox-en.min.8c8ea06bd0420f5067e52fa727b9f92303757322ba4431774153d59a9735eadb.js"
    integrity="sha256-jI6ga9BCD1Bn5S&#43;nJ7n5IwN1cyK6RDF3QVPVmpc16ts="
  ></script>

  
  








  
    
      
      <script async defer src="https://buttons.github.io/buttons.js"></script>

      
    
  




</head>

  <body class="dark:bg-hb-dark dark:text-white page-wrapper" id="top">
    <div id="page-bg"></div>
    <div class="page-header sticky top-0 z-30">
      
      
      
        
        
        
          <header id="site-header" class="header">
  <nav class="navbar px-3 flex justify-left">
    <div class="order-0 h-100">
      
      <a class="navbar-brand" href="/" title="Armstrong website">
        InSeco
      </a>
    </div>
    
    <input id="nav-toggle" type="checkbox" class="hidden" />
    <label
      for="nav-toggle"
      class="order-3 cursor-pointer flex items-center lg:hidden text-dark dark:text-white lg:order-1">
      <svg id="show-button" class="h-6 fill-current block" viewBox="0 0 20 20">
        <title>Open Menu</title>
        <path d="M0 3h20v2H0V3z m0 6h20v2H0V9z m0 6h20v2H0V0z"></path>
      </svg>
      <svg id="hide-button" class="h-6 fill-current hidden" viewBox="0 0 20 20">
        <title>Close Menu</title>
        <polygon
          points="11 9 22 9 22 11 11 11 11 22 9 22 9 11 -2 11 -2 9 9 9 9 -2 11 -2"
          transform="rotate(45 10 10)"></polygon>
      </svg>
    </label>
    

    
    
    <ul
      id="nav-menu"
      class="navbar-nav order-3 hidden lg:flex w-full pb-6 lg:order-1 lg:w-auto lg:space-x-2 lg:pb-0 xl:space-x-8 justify-left
      ">
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/"
        >Bio</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/experience/"
        >Experience</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/#papers"
        >Publications</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/event/"
        >Talks</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/post/"
        >Post</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/projects/"
        >Projects</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/teaching/"
        >Teaching</a
        >
      </li>
      
      
      
    </ul>

    <div class="order-1 ml-auto flex items-center md:order-2 lg:ml-0">

      
      
      
      <button
        aria-label="search"
        class="text-black hover:text-primary  inline-block px-3 text-xl dark:text-white"
        id="search_toggle">
        <svg xmlns="http://www.w3.org/2000/svg" height="16" width="16" viewBox="0 0 512 512" fill="currentColor"><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8 0 45.3s-32.8 12.5-45.3 0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9 0 208S93.1 0 208 0S416 93.1 416 208zM208 352a144 144 0 1 0 0-288 144 144 0 1 0 0 288z"/></svg>
      </button>
      

      
      
      <div class="px-3 text-black hover:text-primary-700 dark:text-white dark:hover:text-primary-300
            [&.active]:font-bold [&.active]:text-black/90 dark:[&.active]:text-white">
        <button class="theme-toggle mt-1" accesskey="t" title="appearance">
          <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
               fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
               stroke-linejoin="round" class="dark:hidden">
            <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
          </svg>
          <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
               fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
               stroke-linejoin="round" class=" dark:block [&:not(dark)]:hidden">
            <circle cx="12" cy="12" r="5"></circle>
            <line x1="12" y1="1" x2="12" y2="3"></line>
            <line x1="12" y1="21" x2="12" y2="23"></line>
            <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
            <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
            <line x1="1" y1="12" x2="3" y2="12"></line>
            <line x1="21" y1="12" x2="23" y2="12"></line>
            <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
            <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
          </svg>
        </button>
      </div>
      

      
      

      
      
    </div>
  </nav>
</header>


<div id="search" class="hidden p-3"></div>


        
      
    </div>
    <div class="page-body  my-10">
      <div class="mx-auto flex max-w-screen-xl">
  

  
  <div class="hb-sidebar-mobile-menu fixed inset-0 z-10 bg-white dark:bg-black/80 hidden"></div>



<aside class="hb-sidebar-container max-lg:[transform:translate3d(0,-100%,0)] lg:sticky">
  
  <div class="px-4 pt-4 lg:hidden">
    
    
  </div>
  <div class="hb-scrollbar lg:h-[calc(100vh-var(--navbar-height))]">
    <ul class="flex flex-col gap-1 lg:hidden">
      
      
        <li class="open"><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/project/"
    
  >Projects
        <span data-hb-sidebar-toggle>
            <svg fill="none" viewBox="0 0 24 24" stroke="currentColor" class="h-[18px] min-w-[18px] rounded-sm p-0.5 hover:bg-gray-800/5 dark:hover:bg-gray-100/5"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" class="origin-center transition-transform rtl:-rotate-180"></path></svg>
        </span>
    </a><div class="ltr:pr-0 overflow-hidden">
        <ul class="hb-sidebar-list"><li class="flex flex-col open"><a
    class="hb-sidebar-custom-link
      sidebar-active-item bg-primary-100 font-semibold text-primary-800 dark:bg-primary-300 dark:text-primary-900"
    href="/project/agentic/"
    
  >Multi-Agent AI Systems
    </a>
  
    <ul class="hb-sidebar-mobile-toc"><li>
              <a
                href="#"
                class="hb-docs-link"
              ></a>
            </li>
          <li>
              <a
                href="#2-key-characteristics-of-agentic-ai-in-multi-environment-systems"
                class="hb-docs-link"
              >&lt;strong&gt;2. Key Characteristics of Agentic AI in Multi-Environment Systems&lt;/strong&gt;</a>
            </li>
          <li>
              <a
                href="#3-types-of-multi-environment-settings"
                class="hb-docs-link"
              >&lt;strong&gt;3. Types of Multi-Environment Settings&lt;/strong&gt;</a>
            </li>
          <li>
              <a
                href="#4-architectural-models-for-multi-environment-ai-systems"
                class="hb-docs-link"
              >&lt;strong&gt;4. Architectural Models for Multi-Environment AI Systems&lt;/strong&gt;</a>
            </li>
          <li>
              <a
                href="#5-security-and-trust-in-agentic-ai"
                class="hb-docs-link"
              >&lt;strong&gt;5. Security and Trust in Agentic AI&lt;/strong&gt;</a>
            </li>
          <li>
              <a
                href="#6-real-world-applications-of-agentic-ai-in-multi-environment-systems"
                class="hb-docs-link"
              >&lt;strong&gt;6. Real-World Applications of Agentic AI in Multi-Environment Systems&lt;/strong&gt;</a>
            </li>
          <li>
              <a
                href="#7-conclusion"
                class="hb-docs-link"
              >&lt;strong&gt;7. Conclusion&lt;/strong&gt;</a>
            </li>
          <li>
              <a
                href="#conclusion"
                class="hb-docs-link"
              >&lt;strong&gt;Conclusion&lt;/strong&gt;</a>
            </li>
          </ul>
  
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/project/climate-change/"
    
  >Climate change
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/project/toxicity/"
    
  >Toxicity and unconcious bias
    </a>
              
            </li></ul>
      </div></li>
        <li class=""><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/projects/"
    
  >Projects
    </a></li>
        <li class=""><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/experience/"
    
  >Experience
    </a></li>
        <li class=""><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/post/"
    
  >Blogging Techky
        <span data-hb-sidebar-toggle>
            <svg fill="none" viewBox="0 0 24 24" stroke="currentColor" class="h-[18px] min-w-[18px] rounded-sm p-0.5 hover:bg-gray-800/5 dark:hover:bg-gray-100/5"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" class="origin-center transition-transform rtl:-rotate-180"></path></svg>
        </span>
    </a><div class="ltr:pr-0 overflow-hidden">
        <ul class="hb-sidebar-list"><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/post/get-started/"
    
  >üéâ Easily create your own simple yet highly customizable blog
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/post/second-brain/"
    
  >üß† Sharpen your thinking with a second brain
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/post/data-visualization/"
    
  >üìà Communicate your results effectively with the best data visualizations
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/post/teach-courses/"
    
  >üë©üèº‚Äçüè´ Teaching technical Skills
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/post/project-management/"
    
  >‚úÖ Manage your projects
    </a>
              
            </li></ul>
      </div></li>
        <li class=""><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/publication/"
    
  >Publications
        <span data-hb-sidebar-toggle>
            <svg fill="none" viewBox="0 0 24 24" stroke="currentColor" class="h-[18px] min-w-[18px] rounded-sm p-0.5 hover:bg-gray-800/5 dark:hover:bg-gray-100/5"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" class="origin-center transition-transform rtl:-rotate-180"></path></svg>
        </span>
    </a><div class="ltr:pr-0 overflow-hidden">
        <ul class="hb-sidebar-list"><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/publication/openja-2024-empirical/"
    
  >An empirical study of testing machine learning in the wild
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/publication/taraghi-2024-deep/"
    
  >Deep learning model reuse in the huggingface community: Challenges, benefit and trends
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/publication/foundjem-2023-grounded/"
    
  >A Grounded Theory of Cross-community SECOs: Feedback Diversity vs. Synchronization
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/publication/openja-2023-studying/"
    
  >Studying the Practices of Testing Machine Learning Software in the Wild
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/publication/foundjem-2022-mixed/"
    
  >A mixed-methods analysis of micro-collaborative coding practices in OpenStack
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/publication/foundjem-2022-software/"
    
  >Software Ecosystem Sustainability, a Socio-Technical Perspective
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/publication/foundjem-2021-open/"
    
  >An open dataset for onboarding new contributors: empirical study of openstack ecosystem
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/publication/foundjem-2021-onboarding/"
    
  >Onboarding vs. diversity, productivity and quality‚Äîempirical study of the openstack ecosystem
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/publication/foundjem-2021-release/"
    
  >Release synchronization in software ecosystems: Empirical Study on OpenStack
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/publication/foundjem-2020-cross/"
    
  >Cross-distribution feedback in software ecosystems
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/publication/foundjem-2019-release/"
    
  >Release synchronization in software ecosystems
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/publication/armstrong-2017-broadcast/"
    
  >Broadcast vs. unicast review technology: Does it matter?
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/publication/foundjem-2017-towards/"
    
  >Towards improving the reliability of live migration operations in OpenStack clouds
    </a>
              
            </li></ul>
      </div></li>
        <li class=""><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/event/"
    
  >Recent Talks
        <span data-hb-sidebar-toggle>
            <svg fill="none" viewBox="0 0 24 24" stroke="currentColor" class="h-[18px] min-w-[18px] rounded-sm p-0.5 hover:bg-gray-800/5 dark:hover:bg-gray-100/5"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" class="origin-center transition-transform rtl:-rotate-180"></path></svg>
        </span>
    </a><div class="ltr:pr-0 overflow-hidden">
        <ul class="hb-sidebar-list"><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/event/openinfra25/"
    
  >Leveraging OpenInfra and Open Source GenAI to Address Climate Change
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/event/openinfra23/"
    
  >How large language and deep learning models can prevent toxicity such as unconscious biases
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/event/fse21/"
    
  >ESEC/FSE21 --- Release Synchronization in Software Ecosystems
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/event/icse21/"
    
  >ICSE&#39;21 --- Onboarding vs. Diversity, Productivity and Quality ‚Äî Empirical Study of the OpenStack Ecosystem
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/event/podcast/"
    
  >CHAOSScast
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/event/icst17/"
    
  >ICST&#39;17 ---Broadcast vs. Unicast Review Technology: Does It Matter?
    </a>
              
            </li></ul>
      </div></li>
        <li class=""><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/students/"
    
  >Students
    </a></li>
        <li class=""><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/teaching/"
    
  >Teaching
        <span data-hb-sidebar-toggle>
            <svg fill="none" viewBox="0 0 24 24" stroke="currentColor" class="h-[18px] min-w-[18px] rounded-sm p-0.5 hover:bg-gray-800/5 dark:hover:bg-gray-100/5"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" class="origin-center transition-transform rtl:-rotate-180"></path></svg>
        </span>
    </a><div class="ltr:pr-0 overflow-hidden">
        <ul class="hb-sidebar-list"><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/teaching/quality-eng/"
    
  >Software Quality Engineering
    </a>
              
            </li><li class="flex flex-col "><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/teaching/security/"
    
  >Cloud Security
    </a>
              
            </li></ul>
      </div></li>
    </ul>

    <ul class="flex flex-col gap-1 max-lg:hidden">
        
        <li class="open"><a
    class="hb-sidebar-custom-link
      sidebar-active-item bg-primary-100 font-semibold text-primary-800 dark:bg-primary-300 dark:text-primary-900"
    href="/project/agentic/"
    
  >Multi-Agent AI Systems
    </a></li>
        <li class=""><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/project/climate-change/"
    
  >Climate change
    </a></li>
        <li class=""><a
    class="hb-sidebar-custom-link
      text-gray-500 hover:bg-gray-100 hover:text-gray-900 dark:text-gray-300 dark:hover:bg-primary-800 dark:hover:text-gray-50"
    href="/project/toxicity/"
    
  >Toxicity and unconcious bias
    </a></li>
        
      </ul>
    </div>

</aside>
  

<nav class="hb-toc order-last hidden w-64 shrink-0 xl:block print:hidden px-4" aria-label="table of contents">
  











  <div class="hb-scrollbar text-sm [hyphens:auto] sticky top-16 overflow-y-auto pr-4 pt-6 max-h-[calc(100vh-var(--navbar-height)-env(safe-area-inset-bottom))] -mr-4 rtl:-ml-4"><p class="mb-4 font-semibold tracking-tight">On this page</p><ul>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="pl-8 rtl:pr-8 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#1-introduction-to-agentic-ai">1. Introduction to Agentic AI</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="font-semibold inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#2-key-characteristics-of-agentic-ai-in-multi-environment-systems">2. Key Characteristics of Agentic AI in Multi-Environment Systems</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="font-semibold inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#3-types-of-multi-environment-settings">3. Types of Multi-Environment Settings</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="pl-4 rtl:pr-4 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#31-homogeneous-vs-heterogeneous-environments">3.1. Homogeneous vs. Heterogeneous Environments</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="pl-4 rtl:pr-4 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#32-static-vs-dynamic-environments">3.2. Static vs. Dynamic Environments</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="pl-4 rtl:pr-4 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#33-cooperative-vs-competitive-multi-agent-environments">3.3. Cooperative vs. Competitive Multi-Agent Environments</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="pl-4 rtl:pr-4 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#34-fully-observable-vs-partially-observable-environments">3.4. Fully Observable vs. Partially Observable Environments</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="font-semibold inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#4-architectural-models-for-multi-environment-ai-systems">4. Architectural Models for Multi-Environment AI Systems</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="pl-4 rtl:pr-4 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#41-multi-agent-reinforcement-learning-marl">4.1. Multi-Agent Reinforcement Learning (MARL)</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="pl-4 rtl:pr-4 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#42-decentralized-partially-observable-markov-decision-processes-dec-pomdp">4.2. Decentralized Partially Observable Markov Decision Processes (Dec-POMDP)</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="pl-4 rtl:pr-4 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#43-federated-learning-for-distributed-ai-agents">4.3. Federated Learning for Distributed AI Agents</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="font-semibold inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#5-security-and-trust-in-agentic-ai">5. Security and Trust in Agentic AI</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="pl-4 rtl:pr-4 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#51-adversarial-ai-attacks">5.1. Adversarial AI Attacks</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="pl-4 rtl:pr-4 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#52-trust-models">5.2. Trust Models</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="font-semibold inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#6-real-world-applications-of-agentic-ai-in-multi-environment-systems">6. Real-World Applications of Agentic AI in Multi-Environment Systems</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="font-semibold inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#7-conclusion">7. Conclusion</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="pl-4 rtl:pr-4 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#1-multi-agent-reinforcement-learning-marl">1. Multi-Agent Reinforcement Learning (MARL)</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="pl-8 rtl:pr-8 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#environment">Environment:</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="pl-8 rtl:pr-8 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#code">Code:</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="pl-8 rtl:pr-8 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#explanation">Explanation:</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="pl-4 rtl:pr-4 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#2-game-theory-based-multi-agent-decision-making">2. Game Theory-Based Multi-Agent Decision Making</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="pl-8 rtl:pr-8 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#explanation-1">Explanation:</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="pl-4 rtl:pr-4 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#3-decentralized-multi-agent-system-with-communication">3. Decentralized Multi-Agent System with Communication</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="pl-8 rtl:pr-8 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#explanation-2">Explanation:</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="pl-4 rtl:pr-4 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#4-reinforcement-learning-for-multi-agent-traffic-control">4. Reinforcement Learning for Multi-Agent Traffic Control</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="pl-8 rtl:pr-8 inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#explanation-3">Explanation:</a>
      </li>
      <li class="my-2 scroll-my-6 scroll-py-6">
        <a class="font-semibold inline-block text-gray-500 hover:text-gray-900 dark:text-gray-400 dark:hover:text-gray-300 w-full break-words" href="#conclusion">Conclusion</a>
      </li></ul>

  
  
    
    
  



    












  </div>
  </nav>


  <article class="flex w-full min-w-0 min-h-[calc(100vh-var(--navbar-height))] justify-center break-words pb-8 pr-[calc(env(safe-area-inset-right)-1.5rem)]">
    <main class="prose prose-slate lg:prose-xl dark:prose-invert w-full min-w-0 max-w-6xl px-6 pt-4 md:px-12">
      <div class="mb-1">
        <div class="mt-1.5 flex items-center gap-1 overflow-hidden text-sm text-gray-500 dark:text-gray-400">
      <div class="whitespace-nowrap transition-colors min-w-[24px] overflow-hidden text-ellipsis hover:text-gray-900 dark:hover:text-gray-100">
        <a href="https://foundjem.github.io/project/">Projects</a>
      </div><svg class="w-3.5 shrink-0" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m8.25 4.5l7.5 7.5l-7.5 7.5"/></svg><div class="whitespace-nowrap font-medium text-gray-700 dark:text-gray-100">Multi-Agent AI Systems</div>
</div>

      </div>

      <div class="content text-base">
        <h1>Multi-Agent AI Systems</h1>
        <h1 id="agentic-ai-systems-in-multi-environment-settings"><strong>Agentic AI Systems in Multi-Environment Settings</strong></h1>
<h4 id="1-introduction-to-agentic-ai"><strong>1. Introduction to Agentic AI</strong></h4>
<p>An <strong>Agentic AI System</strong> refers to an autonomous AI system that can sense, decide, and act in an environment to achieve specific goals. In <strong>multi-environment settings</strong>, these AI agents operate across diverse, dynamic, and often conflicting environments, requiring <strong>adaptive decision-making, communication, and coordination</strong>.</p>
<hr>
<h2 id="2-key-characteristics-of-agentic-ai-in-multi-environment-systems"><strong>2. Key Characteristics of Agentic AI in Multi-Environment Systems</strong></h2>
<ol>
<li><strong>Autonomy</strong> ‚Äì Operates independently with minimal human intervention.</li>
<li><strong>Adaptability</strong> ‚Äì Adjusts behavior based on real-time environmental changes.</li>
<li><strong>Multi-Agent Coordination</strong> ‚Äì Collaborates or competes with other agents.</li>
<li><strong>Distributed Decision-Making</strong> ‚Äì Decentralized intelligence for resilience.</li>
<li><strong>Goal-Oriented Optimization</strong> ‚Äì Maximizes rewards while minimizing risks.</li>
</ol>
<hr>
<h2 id="3-types-of-multi-environment-settings"><strong>3. Types of Multi-Environment Settings</strong></h2>
<h3 id="31-homogeneous-vs-heterogeneous-environments"><strong>3.1. Homogeneous vs. Heterogeneous Environments</strong></h3>
<ul>
<li><strong>Homogeneous Environments</strong>: AI agents operate in uniform conditions (e.g., cloud-based automation).</li>
<li><strong>Heterogeneous Environments</strong>: Agents interact in mixed conditions with different rules, constraints, and uncertainties (e.g., cyber-physical systems).</li>
</ul>
<h3 id="32-static-vs-dynamic-environments"><strong>3.2. Static vs. Dynamic Environments</strong></h3>
<ul>
<li><strong>Static Environments</strong>: Rules and conditions remain constant (e.g., financial AI trading models).</li>
<li><strong>Dynamic Environments</strong>: Conditions evolve over time (e.g., self-driving cars in urban traffic).</li>
</ul>
<h3 id="33-cooperative-vs-competitive-multi-agent-environments"><strong>3.3. Cooperative vs. Competitive Multi-Agent Environments</strong></h3>
<ul>
<li><strong>Cooperative</strong>: AI agents work together towards a shared goal (e.g., swarm robotics in disaster response).</li>
<li><strong>Competitive</strong>: AI agents compete against each other (e.g., adversarial cybersecurity AI).</li>
</ul>
<h3 id="34-fully-observable-vs-partially-observable-environments"><strong>3.4. Fully Observable vs. Partially Observable Environments</strong></h3>
<ul>
<li><strong>Fully Observable</strong>: AI agents have complete visibility (e.g., chess AI).</li>
<li><strong>Partially Observable</strong>: AI agents make decisions with limited information (e.g., autonomous drones in complex terrain).</li>
</ul>
<hr>
<h2 id="4-architectural-models-for-multi-environment-ai-systems"><strong>4. Architectural Models for Multi-Environment AI Systems</strong></h2>
<h3 id="41-multi-agent-reinforcement-learning-marl"><strong>4.1. Multi-Agent Reinforcement Learning (MARL)</strong></h3>
<ul>
<li>Agents learn optimal strategies through interaction and rewards.</li>
<li><strong>Mathematical Model:</strong>
\[
  Q(s, a) = (1 - \alpha) Q(s, a) + \alpha [R + \gamma \max_{a'} Q(s', a')]
  \]
<ul>
<li>\( Q(s, a) \): Expected reward for action \( a \) in state \( s \)</li>
<li>\( \alpha \): Learning rate</li>
<li>\( R \): Immediate reward</li>
<li>\( \gamma \): Discount factor for future rewards</li>
</ul>
</li>
</ul>
<h3 id="42-decentralized-partially-observable-markov-decision-processes-dec-pomdp"><strong>4.2. Decentralized Partially Observable Markov Decision Processes (Dec-POMDP)</strong></h3>
<ul>
<li>Used in <strong>multi-agent scenarios</strong> with uncertainty.</li>
<li>Each agent \( i \) has a policy \( \pi_i \) that maps local observations \( o_i \) to actions \( a_i \).</li>
<li><strong>Mathematical Model:</strong>
\[
  \pi_i(o_i) = \arg \max_{a_i} \sum_{t} \gamma^t R_i(s_t, a_t)
  \]</li>
</ul>
<h3 id="43-federated-learning-for-distributed-ai-agents"><strong>4.3. Federated Learning for Distributed AI Agents</strong></h3>
<ul>
<li>Agents <strong>collaborate</strong> by training models locally and sharing updates.</li>
<li><strong>Mathematical Model:</strong>
\[
  w_{t+1} = w_t - \eta \nabla F(w_t)
  \]
<ul>
<li>\( w_t \): Model weights at time \( t \)</li>
<li>\( \eta \): Learning rate</li>
<li>\( \nabla F(w_t) \): Gradient of the loss function</li>
</ul>
</li>
</ul>
<hr>
<h2 id="5-security-and-trust-in-agentic-ai"><strong>5. Security and Trust in Agentic AI</strong></h2>
<h3 id="51-adversarial-ai-attacks"><strong>5.1. Adversarial AI Attacks</strong></h3>
<ul>
<li><strong>Evasion Attacks</strong>: Fooling agents using adversarial examples.</li>
<li><strong>Poisoning Attacks</strong>: Manipulating training data to corrupt decision-making.</li>
</ul>
<h3 id="52-trust-models"><strong>5.2. Trust Models</strong></h3>
<ul>
<li>Trust is modeled using <strong>Bayesian belief networks</strong>:
\[
  P(T | E) = \frac{P(E | T) P(T)}{P(E)}
  \]
where \( P(T | E) \) is the trust probability given evidence \( E \).</li>
</ul>
<hr>
<h2 id="6-real-world-applications-of-agentic-ai-in-multi-environment-systems"><strong>6. Real-World Applications of Agentic AI in Multi-Environment Systems</strong></h2>
<ol>
<li><strong>Autonomous Vehicles</strong> ‚Äì Navigate in <strong>dynamic, multi-agent</strong> urban traffic.</li>
<li><strong>Cybersecurity AI</strong> ‚Äì Detect threats in <strong>partially observable</strong> network environments.</li>
<li><strong>Healthcare AI</strong> ‚Äì <strong>Federated learning</strong> for personalized medicine.</li>
<li><strong>Financial AI Trading</strong> ‚Äì <strong>Reinforcement learning-based</strong> market strategies.</li>
<li><strong>Smart Grid Energy Management</strong> ‚Äì Adaptive <strong>multi-agent</strong> optimization.</li>
</ol>
<hr>
<h2 id="7-conclusion"><strong>7. Conclusion</strong></h2>
<p>Agentic AI in multi-environment settings requires:</p>
<ul>
<li><strong>Adaptive learning models</strong> (MARL, Dec-POMDP).</li>
<li><strong>Distributed decision-making</strong> (Federated AI).</li>
<li><strong>Security mechanisms</strong> (Trust models, Adversarial AI).
These <strong>autonomous systems</strong> will drive the future of <strong>self-learning, secure, and efficient AI ecosystems</strong>. üöÄ</li>
</ul>
<hr>
<h3 id="1-multi-agent-reinforcement-learning-marl"><strong>1. Multi-Agent Reinforcement Learning (MARL)</strong></h3>
<p>This example implements <strong>Q-learning</strong> for two agents navigating a grid environment.</p>
<h4 id="environment"><strong>Environment:</strong></h4>
<ul>
<li>A <strong>5x5 grid</strong> where two agents must reach their respective goals.</li>
<li><strong>Reward:</strong> +10 for reaching the goal, -1 for illegal moves.</li>
<li><strong>Agents learn simultaneously using Q-learning</strong>.</li>
</ul>
<h4 id="code"><strong>Code:</strong></h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">random</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Environment settings</span>
</span></span><span class="line"><span class="cl"><span class="n">GRID_SIZE</span> <span class="o">=</span> <span class="mi">5</span>
</span></span><span class="line"><span class="cl"><span class="n">ACTIONS</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;UP&#34;</span><span class="p">,</span> <span class="s2">&#34;DOWN&#34;</span><span class="p">,</span> <span class="s2">&#34;LEFT&#34;</span><span class="p">,</span> <span class="s2">&#34;RIGHT&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">ACTION_MAP</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;UP&#34;</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="s2">&#34;DOWN&#34;</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="s2">&#34;LEFT&#34;</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="s2">&#34;RIGHT&#34;</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Agents start at random positions, goals at fixed points</span>
</span></span><span class="line"><span class="cl"><span class="n">AGENT_1_GOAL</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">AGENT_2_GOAL</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Q-tables for agents</span>
</span></span><span class="line"><span class="cl"><span class="n">Q_agent1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">GRID_SIZE</span><span class="p">,</span> <span class="n">GRID_SIZE</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">ACTIONS</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl"><span class="n">Q_agent2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">GRID_SIZE</span><span class="p">,</span> <span class="n">GRID_SIZE</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">ACTIONS</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Hyperparameters</span>
</span></span><span class="line"><span class="cl"><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span>  <span class="c1"># Learning rate</span>
</span></span><span class="line"><span class="cl"><span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.9</span>  <span class="c1"># Discount factor</span>
</span></span><span class="line"><span class="cl"><span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.1</span>  <span class="c1"># Exploration rate</span>
</span></span><span class="line"><span class="cl"><span class="n">episodes</span> <span class="o">=</span> <span class="mi">5000</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Function to get next position</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">move</span><span class="p">(</span><span class="n">position</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">new_position</span> <span class="o">=</span> <span class="p">(</span><span class="n">position</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">ACTION_MAP</span><span class="p">[</span><span class="n">action</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">position</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">ACTION_MAP</span><span class="p">[</span><span class="n">action</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">new_position</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">GRID_SIZE</span> <span class="ow">and</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">new_position</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">GRID_SIZE</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">new_position</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">position</span>  <span class="c1"># Invalid moves stay in place</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Training loop</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">episodes</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">agent1_pos</span> <span class="o">=</span> <span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">GRID_SIZE</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">GRID_SIZE</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">agent2_pos</span> <span class="o">=</span> <span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">GRID_SIZE</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">GRID_SIZE</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">while</span> <span class="n">agent1_pos</span> <span class="o">!=</span> <span class="n">AGENT_1_GOAL</span> <span class="ow">or</span> <span class="n">agent2_pos</span> <span class="o">!=</span> <span class="n">AGENT_2_GOAL</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">agent</span><span class="p">,</span> <span class="n">Q_table</span><span class="p">,</span> <span class="n">goal</span> <span class="ow">in</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="n">Q_agent1</span><span class="p">,</span> <span class="n">AGENT_1_GOAL</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">Q_agent2</span><span class="p">,</span> <span class="n">AGENT_2_GOAL</span><span class="p">)]:</span>
</span></span><span class="line"><span class="cl">            <span class="n">pos</span> <span class="o">=</span> <span class="n">agent1_pos</span> <span class="k">if</span> <span class="n">agent</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">agent2_pos</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">pos</span> <span class="o">==</span> <span class="n">goal</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">continue</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># Choose action (Œµ-greedy)</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">action</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">ACTIONS</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">action</span> <span class="o">=</span> <span class="n">ACTIONS</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Q_table</span><span class="p">[</span><span class="n">pos</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">pos</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">:])]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># Move agent</span>
</span></span><span class="line"><span class="cl">            <span class="n">new_pos</span> <span class="o">=</span> <span class="n">move</span><span class="p">(</span><span class="n">pos</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">reward</span> <span class="o">=</span> <span class="mi">10</span> <span class="k">if</span> <span class="n">new_pos</span> <span class="o">==</span> <span class="n">goal</span> <span class="k">else</span> <span class="o">-</span><span class="mi">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># Update Q-table</span>
</span></span><span class="line"><span class="cl">            <span class="n">Q_table</span><span class="p">[</span><span class="n">pos</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">pos</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">ACTIONS</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">action</span><span class="p">)]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">Q_table</span><span class="p">[</span><span class="n">new_pos</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">new_pos</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">:])</span> <span class="o">-</span> <span class="n">Q_table</span><span class="p">[</span><span class="n">pos</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">pos</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">ACTIONS</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">action</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">            <span class="c1"># Update agent position</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">agent</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">agent1_pos</span> <span class="o">=</span> <span class="n">new_pos</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">agent2_pos</span> <span class="o">=</span> <span class="n">new_pos</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Training complete! Agents have learned optimal paths.&#34;</span><span class="p">)</span>
</span></span></code></pre></div><h4 id="explanation"><strong>Explanation:</strong></h4>
<ul>
<li><strong>Two agents learn independently</strong> using <strong>Q-learning</strong>.</li>
<li><strong>Grid-based movement</strong>, avoiding invalid moves.</li>
<li><strong>Goal-oriented reinforcement learning</strong>.</li>
</ul>
<hr>
<h3 id="2-game-theory-based-multi-agent-decision-making"><strong>2. Game Theory-Based Multi-Agent Decision Making</strong></h3>
<p>This example implements a <strong>Prisoner&rsquo;s Dilemma</strong> game between two AI agents.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">nashpy</span> <span class="k">as</span> <span class="nn">nash</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Define the payoff matrix for Prisoner&#39;s Dilemma</span>
</span></span><span class="line"><span class="cl"><span class="n">P1_payoffs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">]])</span>  <span class="c1"># Row player (Agent 1)</span>
</span></span><span class="line"><span class="cl"><span class="n">P2_payoffs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">]])</span>  <span class="c1"># Column player (Agent 2)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create a game using Nashpy</span>
</span></span><span class="line"><span class="cl"><span class="n">game</span> <span class="o">=</span> <span class="n">nash</span><span class="o">.</span><span class="n">Game</span><span class="p">(</span><span class="n">P1_payoffs</span><span class="p">,</span> <span class="n">P2_payoffs</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Compute Nash Equilibria</span>
</span></span><span class="line"><span class="cl"><span class="n">equilibria</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">game</span><span class="o">.</span><span class="n">support_enumeration</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Display Nash Equilibrium Strategies</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Nash Equilibria (Mixed Strategies):&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">eq</span> <span class="ow">in</span> <span class="n">equilibria</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Agent 1 Strategy: </span><span class="si">{</span><span class="n">eq</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">, Agent 2 Strategy: </span><span class="si">{</span><span class="n">eq</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span></code></pre></div><h4 id="explanation-1"><strong>Explanation:</strong></h4>
<ul>
<li>Models two <strong>self-interested agents</strong> choosing <strong>cooperate (C) or defect (D)</strong>.</li>
<li><strong>Nash equilibrium</strong> represents the <strong>optimal mixed strategies</strong>.</li>
</ul>
<hr>
<h3 id="3-decentralized-multi-agent-system-with-communication"><strong>3. Decentralized Multi-Agent System with Communication</strong></h3>
<p>A <strong>swarm of agents</strong> moves towards a goal using <strong>decentralized coordination</strong>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Environment settings</span>
</span></span><span class="line"><span class="cl"><span class="n">NUM_AGENTS</span> <span class="o">=</span> <span class="mi">10</span>
</span></span><span class="line"><span class="cl"><span class="n">GOAL</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">MOVE_STEP</span> <span class="o">=</span> <span class="mf">0.5</span>
</span></span><span class="line"><span class="cl"><span class="n">COMMUNICATION_RANGE</span> <span class="o">=</span> <span class="mf">2.0</span>
</span></span><span class="line"><span class="cl"><span class="n">ITERATIONS</span> <span class="o">=</span> <span class="mi">50</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Initialize agent positions randomly</span>
</span></span><span class="line"><span class="cl"><span class="n">agents</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">NUM_AGENTS</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="mi">10</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">move_towards_goal</span><span class="p">(</span><span class="n">agent</span><span class="p">,</span> <span class="n">neighbors</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Compute average neighbor position (consensus rule)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">neighbors</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">avg_pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">neighbors</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">move_direction</span> <span class="o">=</span> <span class="n">avg_pos</span> <span class="o">-</span> <span class="n">agent</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">move_direction</span> <span class="o">=</span> <span class="n">GOAL</span> <span class="o">-</span> <span class="n">agent</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">agent</span> <span class="o">+</span> <span class="n">MOVE_STEP</span> <span class="o">*</span> <span class="p">(</span><span class="n">move_direction</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">move_direction</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Simulation</span>
</span></span><span class="line"><span class="cl"><span class="n">positions</span> <span class="o">=</span> <span class="p">[</span><span class="n">agents</span><span class="o">.</span><span class="n">copy</span><span class="p">()]</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ITERATIONS</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">new_positions</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">agent</span> <span class="ow">in</span> <span class="n">agents</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">neighbors</span> <span class="o">=</span> <span class="p">[</span><span class="n">other</span> <span class="k">for</span> <span class="n">other</span> <span class="ow">in</span> <span class="n">agents</span> <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">agent</span> <span class="o">-</span> <span class="n">other</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">COMMUNICATION_RANGE</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">new_positions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">move_towards_goal</span><span class="p">(</span><span class="n">agent</span><span class="p">,</span> <span class="n">neighbors</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="n">agents</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">new_positions</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">positions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">agents</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Plot results</span>
</span></span><span class="line"><span class="cl"><span class="n">positions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">positions</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">NUM_AGENTS</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">positions</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">positions</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">GOAL</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">GOAL</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&#34;X&#34;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&#34;red&#34;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&#34;Goal&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&#34;X Position&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&#34;Y Position&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&#34;Swarm Agents Moving Towards Goal&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><h4 id="explanation-2"><strong>Explanation:</strong></h4>
<ul>
<li><strong>10 agents</strong> move towards a <strong>goal</strong> using <strong>local communication</strong>.</li>
<li><strong>Consensus-based movement</strong> makes it <strong>robust</strong> to missing data.</li>
<li>Models <strong>swarm robotics, decentralized AI, and self-organizing systems</strong>.</li>
</ul>
<hr>
<h3 id="4-reinforcement-learning-for-multi-agent-traffic-control"><strong>4. Reinforcement Learning for Multi-Agent Traffic Control</strong></h3>
<p>A <strong>multi-agent reinforcement learning</strong> setup where <strong>traffic lights learn</strong> optimal control.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">random</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Environment setup</span>
</span></span><span class="line"><span class="cl"><span class="n">ACTIONS</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;RED&#34;</span><span class="p">,</span> <span class="s2">&#34;GREEN&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">TRAFFIC_STATES</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;LOW&#34;</span><span class="p">,</span> <span class="s2">&#34;MEDIUM&#34;</span><span class="p">,</span> <span class="s2">&#34;HIGH&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">Q_table</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">TRAFFIC_STATES</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">ACTIONS</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Learning parameters</span>
</span></span><span class="line"><span class="cl"><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span>
</span></span><span class="line"><span class="cl"><span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.9</span>
</span></span><span class="line"><span class="cl"><span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.1</span>
</span></span><span class="line"><span class="cl"><span class="n">episodes</span> <span class="o">=</span> <span class="mi">5000</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Reward function (based on congestion reduction)</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">reward</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">state</span> <span class="o">==</span> <span class="s2">&#34;HIGH&#34;</span> <span class="ow">and</span> <span class="n">action</span> <span class="o">==</span> <span class="s2">&#34;GREEN&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="mi">10</span>  <span class="c1"># Best action</span>
</span></span><span class="line"><span class="cl">    <span class="k">elif</span> <span class="n">state</span> <span class="o">==</span> <span class="s2">&#34;LOW&#34;</span> <span class="ow">and</span> <span class="n">action</span> <span class="o">==</span> <span class="s2">&#34;RED&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="mi">5</span>  <span class="c1"># Minor reward</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="o">-</span><span class="mi">5</span>  <span class="c1"># Wrong action penalty</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Training</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">episodes</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">state_idx</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">TRAFFIC_STATES</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Choose action (Œµ-greedy)</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">action_idx</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">ACTIONS</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">action_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Q_table</span><span class="p">[</span><span class="n">state_idx</span><span class="p">,</span> <span class="p">:])</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Get reward</span>
</span></span><span class="line"><span class="cl">    <span class="n">r</span> <span class="o">=</span> <span class="n">reward</span><span class="p">(</span><span class="n">TRAFFIC_STATES</span><span class="p">[</span><span class="n">state_idx</span><span class="p">],</span> <span class="n">ACTIONS</span><span class="p">[</span><span class="n">action_idx</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># Update Q-table</span>
</span></span><span class="line"><span class="cl">    <span class="n">Q_table</span><span class="p">[</span><span class="n">state_idx</span><span class="p">,</span> <span class="n">action_idx</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="n">r</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">Q_table</span><span class="p">[</span><span class="n">state_idx</span><span class="p">,</span> <span class="p">:])</span> <span class="o">-</span> <span class="n">Q_table</span><span class="p">[</span><span class="n">state_idx</span><span class="p">,</span> <span class="n">action_idx</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Multi-Agent Traffic Control Training Complete!&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Final Q-Table:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">Q_table</span><span class="p">)</span>
</span></span></code></pre></div><h4 id="explanation-3"><strong>Explanation:</strong></h4>
<ul>
<li><strong>Traffic signals</strong> learn <strong>optimal switching</strong> using <strong>Q-learning</strong>.</li>
<li><strong>Adaptive control</strong> based on <strong>real-time congestion</strong>.</li>
</ul>
<hr>
<h2 id="conclusion"><strong>Conclusion</strong></h2>
<p>These <strong>multi-agent system implementations</strong> provide:</p>
<ol>
<li><strong>Reinforcement Learning (Q-learning)</strong> ‚Äì AI agents learning in a <strong>shared environment</strong>.</li>
<li><strong>Game Theory (Nash Equilibrium)</strong> ‚Äì Competitive or cooperative decision-making.</li>
<li><strong>Decentralized Coordination</strong> ‚Äì Swarm behavior using <strong>local communication</strong>.</li>
<li><strong>Traffic Optimization</strong> ‚Äì AI-based <strong>autonomous traffic control</strong>.</li>
</ol>
<p>These techniques can be used for <strong>robotics, cybersecurity, and distributed AI</strong> applications. üöÄ</p>

      </div>

      
  <time class="mt-12 mb-8 block text-xs text-gray-500 ltr:text-right rtl:text-left dark:text-gray-400" datetime="2025-02-15T11:49:13.000Z">
    <span>Last updated on</span>
    Feb 15, 2025</time>
      
      
  
    
    
    
      
      
    
<div class="pt-1 no-prose w-full">
  <hr class="border-dotted border-neutral-300 dark:border-neutral-600" />
  <div class="flex flex-col md:flex-row flex-nowrap justify-between gap-5 pt-2">
    <div class="">
      
    </div>
    <div class="">
      
        <a class="group flex text-right no-underline" href="/project/climate-change/">
          <span class="flex flex-col">
            <span
              class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500"
            >Climate change</span
            >
            <span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400">
              
                Jan 26, 2025
              
            </span>
          </span>
          <span
            class="mt-[-0.3rem] ms-2 text-neutral-700 transition-transform group-hover:-translate-x-[2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"
          ><span class="ltr:inline">&rarr;</span></span>
        </a>
      
    </div>
  </div>
</div>



      



    </main>
  </article>
</div>

    </div>
    <div class="page-footer">
      <footer class="container mx-auto flex flex-col justify-items-center text-sm leading-6 mt-24 mb-4 text-slate-700 dark:text-slate-200">

  












  
  
  
  
  














  
  
  

  
  
    
  
  
    
  

  

  
  <p class="powered-by text-center">
    ¬© 2025 Me. This work is licensed under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank">CC BY NC ND 4.0</a>
  </p>
  

  <p class="powered-by footer-license-icons">
    <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank" aria-label="Creative Commons">
      <i class="fab fa-creative-commons fa-2x" aria-hidden="true"></i>
      <i class="fab fa-creative-commons-by fa-2x" aria-hidden="true"></i>
      
        <i class="fab fa-creative-commons-nc fa-2x" aria-hidden="true"></i>
      
      
        <i class="fab fa-creative-commons-nd fa-2x" aria-hidden="true"></i>
      
    </a>
  </p>





  <p class="powered-by text-center">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://hugoblox.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Hugo Blox Builder</a> ‚Äî the free, <a href="https://github.com/HugoBlox/hugo-blox-builder" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>

</footer>

    </div>

    
    











  </body>
</html>
