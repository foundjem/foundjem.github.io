<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Agentic AI | Armstrong website</title><link>https://foundjem.github.io/tags/agentic-ai/</link><atom:link href="https://foundjem.github.io/tags/agentic-ai/index.xml" rel="self" type="application/rss+xml"/><description>Agentic AI</description><generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Tue, 11 Feb 2025 00:00:00 +0000</lastBuildDate><image><url>https://foundjem.github.io/media/logo.svg</url><title>Agentic AI</title><link>https://foundjem.github.io/tags/agentic-ai/</link></image><item><title>Multi-Agent AI Systems</title><link>https://foundjem.github.io/project/agentic/</link><pubDate>Tue, 11 Feb 2025 00:00:00 +0000</pubDate><guid>https://foundjem.github.io/project/agentic/</guid><description>&lt;h1 id="agentic-ai-systems-in-multi-environment-settings">&lt;strong>Agentic AI Systems in Multi-Environment Settings&lt;/strong>&lt;/h1>
&lt;h4 id="1-introduction-to-agentic-ai">&lt;strong>1. Introduction to Agentic AI&lt;/strong>&lt;/h4>
&lt;p>An &lt;strong>Agentic AI System&lt;/strong> refers to an autonomous AI system that can sense, decide, and act in an environment to achieve specific goals. In &lt;strong>multi-environment settings&lt;/strong>, these AI agents operate across diverse, dynamic, and often conflicting environments, requiring &lt;strong>adaptive decision-making, communication, and coordination&lt;/strong>.&lt;/p>
&lt;hr>
&lt;h2 id="2-key-characteristics-of-agentic-ai-in-multi-environment-systems">&lt;strong>2. Key Characteristics of Agentic AI in Multi-Environment Systems&lt;/strong>&lt;/h2>
&lt;ol>
&lt;li>&lt;strong>Autonomy&lt;/strong> â€“ Operates independently with minimal human intervention.&lt;/li>
&lt;li>&lt;strong>Adaptability&lt;/strong> â€“ Adjusts behavior based on real-time environmental changes.&lt;/li>
&lt;li>&lt;strong>Multi-Agent Coordination&lt;/strong> â€“ Collaborates or competes with other agents.&lt;/li>
&lt;li>&lt;strong>Distributed Decision-Making&lt;/strong> â€“ Decentralized intelligence for resilience.&lt;/li>
&lt;li>&lt;strong>Goal-Oriented Optimization&lt;/strong> â€“ Maximizes rewards while minimizing risks.&lt;/li>
&lt;/ol>
&lt;hr>
&lt;h2 id="3-types-of-multi-environment-settings">&lt;strong>3. Types of Multi-Environment Settings&lt;/strong>&lt;/h2>
&lt;h3 id="31-homogeneous-vs-heterogeneous-environments">&lt;strong>3.1. Homogeneous vs. Heterogeneous Environments&lt;/strong>&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Homogeneous Environments&lt;/strong>: AI agents operate in uniform conditions (e.g., cloud-based automation).&lt;/li>
&lt;li>&lt;strong>Heterogeneous Environments&lt;/strong>: Agents interact in mixed conditions with different rules, constraints, and uncertainties (e.g., cyber-physical systems).&lt;/li>
&lt;/ul>
&lt;h3 id="32-static-vs-dynamic-environments">&lt;strong>3.2. Static vs. Dynamic Environments&lt;/strong>&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Static Environments&lt;/strong>: Rules and conditions remain constant (e.g., financial AI trading models).&lt;/li>
&lt;li>&lt;strong>Dynamic Environments&lt;/strong>: Conditions evolve over time (e.g., self-driving cars in urban traffic).&lt;/li>
&lt;/ul>
&lt;h3 id="33-cooperative-vs-competitive-multi-agent-environments">&lt;strong>3.3. Cooperative vs. Competitive Multi-Agent Environments&lt;/strong>&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Cooperative&lt;/strong>: AI agents work together towards a shared goal (e.g., swarm robotics in disaster response).&lt;/li>
&lt;li>&lt;strong>Competitive&lt;/strong>: AI agents compete against each other (e.g., adversarial cybersecurity AI).&lt;/li>
&lt;/ul>
&lt;h3 id="34-fully-observable-vs-partially-observable-environments">&lt;strong>3.4. Fully Observable vs. Partially Observable Environments&lt;/strong>&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Fully Observable&lt;/strong>: AI agents have complete visibility (e.g., chess AI).&lt;/li>
&lt;li>&lt;strong>Partially Observable&lt;/strong>: AI agents make decisions with limited information (e.g., autonomous drones in complex terrain).&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="4-architectural-models-for-multi-environment-ai-systems">&lt;strong>4. Architectural Models for Multi-Environment AI Systems&lt;/strong>&lt;/h2>
&lt;h3 id="41-multi-agent-reinforcement-learning-marl">&lt;strong>4.1. Multi-Agent Reinforcement Learning (MARL)&lt;/strong>&lt;/h3>
&lt;ul>
&lt;li>Agents learn optimal strategies through interaction and rewards.&lt;/li>
&lt;li>&lt;strong>Mathematical Model:&lt;/strong>
\[
Q(s, a) = (1 - \alpha) Q(s, a) + \alpha [R + \gamma \max_{a'} Q(s', a')]
\]
&lt;ul>
&lt;li>\( Q(s, a) \): Expected reward for action \( a \) in state \( s \)&lt;/li>
&lt;li>\( \alpha \): Learning rate&lt;/li>
&lt;li>\( R \): Immediate reward&lt;/li>
&lt;li>\( \gamma \): Discount factor for future rewards&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="42-decentralized-partially-observable-markov-decision-processes-dec-pomdp">&lt;strong>4.2. Decentralized Partially Observable Markov Decision Processes (Dec-POMDP)&lt;/strong>&lt;/h3>
&lt;ul>
&lt;li>Used in &lt;strong>multi-agent scenarios&lt;/strong> with uncertainty.&lt;/li>
&lt;li>Each agent \( i \) has a policy \( \pi_i \) that maps local observations \( o_i \) to actions \( a_i \).&lt;/li>
&lt;li>&lt;strong>Mathematical Model:&lt;/strong>
\[
\pi_i(o_i) = \arg \max_{a_i} \sum_{t} \gamma^t R_i(s_t, a_t)
\]&lt;/li>
&lt;/ul>
&lt;h3 id="43-federated-learning-for-distributed-ai-agents">&lt;strong>4.3. Federated Learning for Distributed AI Agents&lt;/strong>&lt;/h3>
&lt;ul>
&lt;li>Agents &lt;strong>collaborate&lt;/strong> by training models locally and sharing updates.&lt;/li>
&lt;li>&lt;strong>Mathematical Model:&lt;/strong>
\[
w_{t+1} = w_t - \eta \nabla F(w_t)
\]
&lt;ul>
&lt;li>\( w_t \): Model weights at time \( t \)&lt;/li>
&lt;li>\( \eta \): Learning rate&lt;/li>
&lt;li>\( \nabla F(w_t) \): Gradient of the loss function&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="5-security-and-trust-in-agentic-ai">&lt;strong>5. Security and Trust in Agentic AI&lt;/strong>&lt;/h2>
&lt;h3 id="51-adversarial-ai-attacks">&lt;strong>5.1. Adversarial AI Attacks&lt;/strong>&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Evasion Attacks&lt;/strong>: Fooling agents using adversarial examples.&lt;/li>
&lt;li>&lt;strong>Poisoning Attacks&lt;/strong>: Manipulating training data to corrupt decision-making.&lt;/li>
&lt;/ul>
&lt;h3 id="52-trust-models">&lt;strong>5.2. Trust Models&lt;/strong>&lt;/h3>
&lt;ul>
&lt;li>Trust is modeled using &lt;strong>Bayesian belief networks&lt;/strong>:
\[
P(T | E) = \frac{P(E | T) P(T)}{P(E)}
\]
where \( P(T | E) \) is the trust probability given evidence \( E \).&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="6-real-world-applications-of-agentic-ai-in-multi-environment-systems">&lt;strong>6. Real-World Applications of Agentic AI in Multi-Environment Systems&lt;/strong>&lt;/h2>
&lt;ol>
&lt;li>&lt;strong>Autonomous Vehicles&lt;/strong> â€“ Navigate in &lt;strong>dynamic, multi-agent&lt;/strong> urban traffic.&lt;/li>
&lt;li>&lt;strong>Cybersecurity AI&lt;/strong> â€“ Detect threats in &lt;strong>partially observable&lt;/strong> network environments.&lt;/li>
&lt;li>&lt;strong>Healthcare AI&lt;/strong> â€“ &lt;strong>Federated learning&lt;/strong> for personalized medicine.&lt;/li>
&lt;li>&lt;strong>Financial AI Trading&lt;/strong> â€“ &lt;strong>Reinforcement learning-based&lt;/strong> market strategies.&lt;/li>
&lt;li>&lt;strong>Smart Grid Energy Management&lt;/strong> â€“ Adaptive &lt;strong>multi-agent&lt;/strong> optimization.&lt;/li>
&lt;/ol>
&lt;hr>
&lt;h2 id="7-conclusion">&lt;strong>7. Conclusion&lt;/strong>&lt;/h2>
&lt;p>Agentic AI in multi-environment settings requires:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Adaptive learning models&lt;/strong> (MARL, Dec-POMDP).&lt;/li>
&lt;li>&lt;strong>Distributed decision-making&lt;/strong> (Federated AI).&lt;/li>
&lt;li>&lt;strong>Security mechanisms&lt;/strong> (Trust models, Adversarial AI).
These &lt;strong>autonomous systems&lt;/strong> will drive the future of &lt;strong>self-learning, secure, and efficient AI ecosystems&lt;/strong>. ðŸš€&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h3 id="1-multi-agent-reinforcement-learning-marl">&lt;strong>1. Multi-Agent Reinforcement Learning (MARL)&lt;/strong>&lt;/h3>
&lt;p>This example implements &lt;strong>Q-learning&lt;/strong> for two agents navigating a grid environment.&lt;/p>
&lt;h4 id="environment">&lt;strong>Environment:&lt;/strong>&lt;/h4>
&lt;ul>
&lt;li>A &lt;strong>5x5 grid&lt;/strong> where two agents must reach their respective goals.&lt;/li>
&lt;li>&lt;strong>Reward:&lt;/strong> +10 for reaching the goal, -1 for illegal moves.&lt;/li>
&lt;li>&lt;strong>Agents learn simultaneously using Q-learning&lt;/strong>.&lt;/li>
&lt;/ul>
&lt;h4 id="code">&lt;strong>Code:&lt;/strong>&lt;/h4>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">numpy&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">np&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">random&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Environment settings&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">GRID_SIZE&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">5&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">ACTIONS&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;UP&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;DOWN&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;LEFT&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;RIGHT&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">ACTION_MAP&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="s2">&amp;#34;UP&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="s2">&amp;#34;DOWN&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="s2">&amp;#34;LEFT&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="s2">&amp;#34;RIGHT&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Agents start at random positions, goals at fixed points&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">AGENT_1_GOAL&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">4&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">4&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">AGENT_2_GOAL&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Q-tables for agents&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Q_agent1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zeros&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">GRID_SIZE&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">GRID_SIZE&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ACTIONS&lt;/span>&lt;span class="p">)))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Q_agent2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zeros&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">GRID_SIZE&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">GRID_SIZE&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ACTIONS&lt;/span>&lt;span class="p">)))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Hyperparameters&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">alpha&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">0.1&lt;/span> &lt;span class="c1"># Learning rate&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">gamma&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">0.9&lt;/span> &lt;span class="c1"># Discount factor&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">epsilon&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">0.1&lt;/span> &lt;span class="c1"># Exploration rate&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">episodes&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">5000&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Function to get next position&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">move&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">position&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">action&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">new_position&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">position&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">ACTION_MAP&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">action&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">position&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">ACTION_MAP&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">action&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="mi">0&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="n">new_position&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">GRID_SIZE&lt;/span> &lt;span class="ow">and&lt;/span> &lt;span class="mi">0&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="n">new_position&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">GRID_SIZE&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">new_position&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">position&lt;/span> &lt;span class="c1"># Invalid moves stay in place&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Training loop&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">episode&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">episodes&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">agent1_pos&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randint&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">GRID_SIZE&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randint&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">GRID_SIZE&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">agent2_pos&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randint&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">GRID_SIZE&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randint&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">GRID_SIZE&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">while&lt;/span> &lt;span class="n">agent1_pos&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="n">AGENT_1_GOAL&lt;/span> &lt;span class="ow">or&lt;/span> &lt;span class="n">agent2_pos&lt;/span> &lt;span class="o">!=&lt;/span> &lt;span class="n">AGENT_2_GOAL&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">agent&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Q_table&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">goal&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="p">[(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Q_agent1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">AGENT_1_GOAL&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Q_agent2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">AGENT_2_GOAL&lt;/span>&lt;span class="p">)]:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pos&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">agent1_pos&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="n">agent&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="k">else&lt;/span> &lt;span class="n">agent2_pos&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">pos&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">goal&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">continue&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Choose action (Îµ-greedy)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">rand&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">epsilon&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">action&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">choice&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ACTIONS&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">action&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ACTIONS&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">argmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Q_table&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">pos&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">pos&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="p">:])]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Move agent&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">new_pos&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">move&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">pos&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">action&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">reward&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">10&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="n">new_pos&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">goal&lt;/span> &lt;span class="k">else&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Update Q-table&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">Q_table&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">pos&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">pos&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">ACTIONS&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">index&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">action&lt;/span>&lt;span class="p">)]&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">alpha&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">reward&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">gamma&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">max&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Q_table&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">new_pos&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">new_pos&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="p">:])&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">Q_table&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">pos&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">pos&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">ACTIONS&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">index&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">action&lt;/span>&lt;span class="p">)]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Update agent position&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">agent&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">agent1_pos&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">new_pos&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">agent2_pos&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">new_pos&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Training complete! Agents have learned optimal paths.&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="explanation">&lt;strong>Explanation:&lt;/strong>&lt;/h4>
&lt;ul>
&lt;li>&lt;strong>Two agents learn independently&lt;/strong> using &lt;strong>Q-learning&lt;/strong>.&lt;/li>
&lt;li>&lt;strong>Grid-based movement&lt;/strong>, avoiding invalid moves.&lt;/li>
&lt;li>&lt;strong>Goal-oriented reinforcement learning&lt;/strong>.&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h3 id="2-game-theory-based-multi-agent-decision-making">&lt;strong>2. Game Theory-Based Multi-Agent Decision Making&lt;/strong>&lt;/h3>
&lt;p>This example implements a &lt;strong>Prisoner&amp;rsquo;s Dilemma&lt;/strong> game between two AI agents.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">nashpy&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">nash&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">numpy&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">np&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Define the payoff matrix for Prisoner&amp;#39;s Dilemma&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">P1_payoffs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">array&lt;/span>&lt;span class="p">([[&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">]])&lt;/span> &lt;span class="c1"># Row player (Agent 1)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">P2_payoffs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">array&lt;/span>&lt;span class="p">([[&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">]])&lt;/span> &lt;span class="c1"># Column player (Agent 2)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Create a game using Nashpy&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">game&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nash&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Game&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">P1_payoffs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">P2_payoffs&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Compute Nash Equilibria&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">equilibria&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">list&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">game&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">support_enumeration&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Display Nash Equilibrium Strategies&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Nash Equilibria (Mixed Strategies):&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">eq&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">equilibria&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="sa">f&lt;/span>&lt;span class="s2">&amp;#34;Agent 1 Strategy: &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">eq&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">, Agent 2 Strategy: &lt;/span>&lt;span class="si">{&lt;/span>&lt;span class="n">eq&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="si">}&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="explanation-1">&lt;strong>Explanation:&lt;/strong>&lt;/h4>
&lt;ul>
&lt;li>Models two &lt;strong>self-interested agents&lt;/strong> choosing &lt;strong>cooperate (C) or defect (D)&lt;/strong>.&lt;/li>
&lt;li>&lt;strong>Nash equilibrium&lt;/strong> represents the &lt;strong>optimal mixed strategies&lt;/strong>.&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h3 id="3-decentralized-multi-agent-system-with-communication">&lt;strong>3. Decentralized Multi-Agent System with Communication&lt;/strong>&lt;/h3>
&lt;p>A &lt;strong>swarm of agents&lt;/strong> moves towards a goal using &lt;strong>decentralized coordination&lt;/strong>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">numpy&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">np&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">matplotlib.pyplot&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">plt&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Environment settings&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">NUM_AGENTS&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">10&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">GOAL&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">array&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mi">5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">5&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">MOVE_STEP&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">0.5&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">COMMUNICATION_RANGE&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">2.0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">ITERATIONS&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">50&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Initialize agent positions randomly&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">agents&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">rand&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">NUM_AGENTS&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">10&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">move_towards_goal&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">agent&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">neighbors&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Compute average neighbor position (consensus rule)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">neighbors&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">avg_pos&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">mean&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">neighbors&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">axis&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">move_direction&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">avg_pos&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">agent&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">move_direction&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">GOAL&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">agent&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">agent&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">MOVE_STEP&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">move_direction&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">linalg&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">norm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">move_direction&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Simulation&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">positions&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">agents&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">copy&lt;/span>&lt;span class="p">()]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">_&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ITERATIONS&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">new_positions&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">agent&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">agents&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">neighbors&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">other&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">other&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">agents&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">linalg&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">norm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">agent&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">other&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">COMMUNICATION_RANGE&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">new_positions&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">move_towards_goal&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">agent&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">neighbors&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">agents&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">array&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">new_positions&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">positions&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">agents&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">copy&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Plot results&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">positions&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">array&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">positions&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">figure&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">figsize&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">6&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">6&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">NUM_AGENTS&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">plot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">positions&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">positions&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">marker&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;o&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">linestyle&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;-&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">scatter&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">GOAL&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">GOAL&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">marker&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;X&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">color&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;red&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">100&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">label&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;Goal&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">xlabel&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;X Position&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ylabel&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Y Position&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">title&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Swarm Agents Moving Towards Goal&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">legend&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">show&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="explanation-2">&lt;strong>Explanation:&lt;/strong>&lt;/h4>
&lt;ul>
&lt;li>&lt;strong>10 agents&lt;/strong> move towards a &lt;strong>goal&lt;/strong> using &lt;strong>local communication&lt;/strong>.&lt;/li>
&lt;li>&lt;strong>Consensus-based movement&lt;/strong> makes it &lt;strong>robust&lt;/strong> to missing data.&lt;/li>
&lt;li>Models &lt;strong>swarm robotics, decentralized AI, and self-organizing systems&lt;/strong>.&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h3 id="4-reinforcement-learning-for-multi-agent-traffic-control">&lt;strong>4. Reinforcement Learning for Multi-Agent Traffic Control&lt;/strong>&lt;/h3>
&lt;p>A &lt;strong>multi-agent reinforcement learning&lt;/strong> setup where &lt;strong>traffic lights learn&lt;/strong> optimal control.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">numpy&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">np&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">random&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Environment setup&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">ACTIONS&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;RED&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;GREEN&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">TRAFFIC_STATES&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;LOW&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;MEDIUM&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;HIGH&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Q_table&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zeros&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">TRAFFIC_STATES&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ACTIONS&lt;/span>&lt;span class="p">)))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Learning parameters&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">alpha&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">0.1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">gamma&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">0.9&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">epsilon&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">0.1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">episodes&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">5000&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Reward function (based on congestion reduction)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">reward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">state&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">action&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">state&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="s2">&amp;#34;HIGH&amp;#34;&lt;/span> &lt;span class="ow">and&lt;/span> &lt;span class="n">action&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="s2">&amp;#34;GREEN&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="mi">10&lt;/span> &lt;span class="c1"># Best action&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">elif&lt;/span> &lt;span class="n">state&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="s2">&amp;#34;LOW&amp;#34;&lt;/span> &lt;span class="ow">and&lt;/span> &lt;span class="n">action&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="s2">&amp;#34;RED&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="mi">5&lt;/span> &lt;span class="c1"># Minor reward&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">5&lt;/span> &lt;span class="c1"># Wrong action penalty&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Training&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">_&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">episodes&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">state_idx&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randint&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">TRAFFIC_STATES&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Choose action (Îµ-greedy)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">rand&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">epsilon&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">action_idx&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randint&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ACTIONS&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">action_idx&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">argmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Q_table&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">state_idx&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Get reward&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">r&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">reward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">TRAFFIC_STATES&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">state_idx&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">ACTIONS&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">action_idx&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># Update Q-table&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">Q_table&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">state_idx&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">action_idx&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">alpha&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">r&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">gamma&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">max&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Q_table&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">state_idx&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:])&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">Q_table&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">state_idx&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">action_idx&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Multi-Agent Traffic Control Training Complete!&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;Final Q-Table:&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Q_table&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="explanation-3">&lt;strong>Explanation:&lt;/strong>&lt;/h4>
&lt;ul>
&lt;li>&lt;strong>Traffic signals&lt;/strong> learn &lt;strong>optimal switching&lt;/strong> using &lt;strong>Q-learning&lt;/strong>.&lt;/li>
&lt;li>&lt;strong>Adaptive control&lt;/strong> based on &lt;strong>real-time congestion&lt;/strong>.&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="conclusion">&lt;strong>Conclusion&lt;/strong>&lt;/h2>
&lt;p>These &lt;strong>multi-agent system implementations&lt;/strong> provide:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Reinforcement Learning (Q-learning)&lt;/strong> â€“ AI agents learning in a &lt;strong>shared environment&lt;/strong>.&lt;/li>
&lt;li>&lt;strong>Game Theory (Nash Equilibrium)&lt;/strong> â€“ Competitive or cooperative decision-making.&lt;/li>
&lt;li>&lt;strong>Decentralized Coordination&lt;/strong> â€“ Swarm behavior using &lt;strong>local communication&lt;/strong>.&lt;/li>
&lt;li>&lt;strong>Traffic Optimization&lt;/strong> â€“ AI-based &lt;strong>autonomous traffic control&lt;/strong>.&lt;/li>
&lt;/ol>
&lt;p>These techniques can be used for &lt;strong>robotics, cybersecurity, and distributed AI&lt;/strong> applications. ðŸš€&lt;/p></description></item></channel></rss>