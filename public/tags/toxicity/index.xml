<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Toxicity | Armstrong website</title><link>https://foundjem.github.io/tags/toxicity/</link><atom:link href="https://foundjem.github.io/tags/toxicity/index.xml" rel="self" type="application/rss+xml"/><description>Toxicity</description><generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Tue, 11 Feb 2025 13:00:00 +0000</lastBuildDate><image><url>https://foundjem.github.io/media/logo.svg</url><title>Toxicity</title><link>https://foundjem.github.io/tags/toxicity/</link></image><item><title>Leveraging OpenInfra and Open Source GenAI to Address Climate Change</title><link>https://foundjem.github.io/event/openinfra25/</link><pubDate>Tue, 11 Feb 2025 13:00:00 +0000</pubDate><guid>https://foundjem.github.io/event/openinfra25/</guid><description>&lt;div class="flex px-4 py-3 mb-6 rounded-md bg-primary-100 dark:bg-primary-900">
&lt;span class="pr-3 pt-1 text-primary-600 dark:text-primary-300">
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z"/>&lt;/svg>
&lt;/span>
&lt;span class="dark:text-neutral-300">Click on the &lt;strong>Slides&lt;/strong> button above to view the built-in slides feature.&lt;/span>
&lt;/div></description></item><item><title>Toxicity and unconcious bias</title><link>https://foundjem.github.io/project/toxicity/</link><pubDate>Thu, 26 Dec 2024 00:00:00 +0000</pubDate><guid>https://foundjem.github.io/project/toxicity/</guid><description>&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/Oni9oaNjsnQ?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video">&lt;/iframe>
&lt;/div>
&lt;h2 id="using-nlp-and-language-models-to-address-toxicity-and-unconscious-bias">&lt;strong>Using NLP and Language Models to Address Toxicity and Unconscious Bias&lt;/strong>&lt;/h2>
&lt;h2 id="theoretical-foundations-of-nlp-for-addressing-toxicity-and-bias">&lt;strong>Theoretical Foundations of NLP for Addressing Toxicity and Bias&lt;/strong>&lt;/h2>
&lt;p>To rigorously address &lt;strong>toxicity and unconscious bias&lt;/strong> in language models, we rely on &lt;strong>linguistic theories, probabilistic models, fairness-aware AI principles, adversarial learning, and information theory&lt;/strong>. Below, I outline the &lt;strong>strong theoretical basis&lt;/strong> underpinning these techniques.&lt;/p>
&lt;hr>
&lt;h2 id="1-toxicity-detection-and-classification">&lt;strong>1. Toxicity Detection and Classification&lt;/strong>&lt;/h2>
&lt;p>Toxicity detection can be modeled as a &lt;strong>probabilistic text classification problem&lt;/strong> where we assign labels (toxic or non-toxic) based on learned representations.&lt;/p>
&lt;h3 id="11-bayesian-formulation-of-text-classification">&lt;strong>1.1. Bayesian Formulation of Text Classification&lt;/strong>&lt;/h3>
&lt;p>Using &lt;strong>NaÃ¯ve Bayes&lt;/strong> for toxicity detection:&lt;/p>
\[
P(T | X) = \frac{P(X | T) P(T)}{P(X)}
\]&lt;p>where:&lt;/p>
&lt;ul>
&lt;li>\( P(T | X) \) is the probability that text \( X \) is toxic.&lt;/li>
&lt;li>\( P(X | T) \) is the likelihood of observing text \( X \) given it is toxic.&lt;/li>
&lt;li>\( P(T) \) is the prior probability of toxicity.&lt;/li>
&lt;li>\( P(X) \) is the probability of observing text \( X \).&lt;/li>
&lt;/ul>
&lt;p>By assuming &lt;strong>independence of words&lt;/strong> (Bag-of-Words model):&lt;/p>
\[
P(T | X) \propto P(T) \prod_{i=1}^{n} P(w_i | T)
\]&lt;p>where \( w_i \) are the words in text \( X \).&lt;/p>
&lt;p>This model is &lt;strong>effective for simple toxicity detection&lt;/strong> but struggles with &lt;strong>context-dependent toxicity&lt;/strong> (e.g., sarcasm).&lt;/p>
&lt;h3 id="12-deep-learning-for-toxicity-classification">&lt;strong>1.2. Deep Learning for Toxicity Classification&lt;/strong>&lt;/h3>
&lt;p>A more robust method is using &lt;strong>deep neural networks (DNNs)&lt;/strong> with word embeddings \( W \) and classification function \( f(W) \):&lt;/p>
\[
y = f(W) = \sigma(W \cdot X + b)
\]&lt;p>where:&lt;/p>
&lt;ul>
&lt;li>\( W \) is the weight matrix,&lt;/li>
&lt;li>\( X \) is the word embedding vector,&lt;/li>
&lt;li>\( \sigma \) is the softmax function.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Optimization Problem&lt;/strong>:
To minimize classification error, we solve:&lt;/p>
\[
\min_{W} \sum_{i=1}^{N} L(y_i, f(W X_i))
\]&lt;p>where \( L \) is a loss function (e.g., cross-entropy loss).&lt;/p>
&lt;hr>
&lt;h2 id="2-bias-detection-in-language-models">&lt;strong>2. Bias Detection in Language Models&lt;/strong>&lt;/h2>
&lt;h3 id="21-word-embedding-association-test-weat">&lt;strong>2.1. Word Embedding Association Test (WEAT)&lt;/strong>&lt;/h3>
&lt;p>To measure bias in word embeddings (e.g., Word2Vec, GloVe), we use &lt;strong>cosine similarity&lt;/strong> to quantify associations.&lt;/p>
&lt;h4 id="mathematical-definition">&lt;strong>Mathematical Definition&lt;/strong>&lt;/h4>
&lt;p>Given &lt;strong>two sets of words&lt;/strong>:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Target set&lt;/strong>: \( T = \{w_1, w_2, ..., w_m\} \)&lt;/li>
&lt;li>&lt;strong>Attribute set&lt;/strong>: \( A = \{a_1, a_2, ..., a_n\} \)&lt;/li>
&lt;/ul>
&lt;p>The bias score is:&lt;/p>
\[
s(T, A) = \sum_{w \in T} \left[ \frac{1}{n} \sum_{a \in A} \cos(w, a) \right]
\]&lt;p>If &lt;strong>gendered words (e.g., &amp;ldquo;man&amp;rdquo;, &amp;ldquo;woman&amp;rdquo;) cluster with career-related terms (e.g., &amp;ldquo;doctor&amp;rdquo;, &amp;ldquo;nurse&amp;rdquo;)&lt;/strong>, this indicates &lt;strong>stereotypical biases&lt;/strong> in embeddings.&lt;/p>
&lt;h4 id="example-bias-in-word-embeddings">&lt;strong>Example: Bias in Word Embeddings&lt;/strong>&lt;/h4>
\[
\text{cosine}(\text{"doctor"}, \text{"man"}) > \text{cosine}(\text{"doctor"}, \text{"woman"})
\]&lt;p>This means &lt;strong>&amp;ldquo;doctor&amp;rdquo; is closer to &amp;ldquo;man&amp;rdquo; than &amp;ldquo;woman&amp;rdquo;&lt;/strong>, reflecting gender bias in training data.&lt;/p>
&lt;h3 id="22-bias-correction-using-orthogonal-projection">&lt;strong>2.2. Bias Correction Using Orthogonal Projection&lt;/strong>&lt;/h3>
&lt;p>To debias embeddings, we project onto a &lt;strong>bias-free subspace&lt;/strong>:&lt;/p>
\[
\tilde{w} = w - \sum_{i=1}^{k} \langle w, b_i \rangle b_i
\]&lt;p>where:&lt;/p>
&lt;ul>
&lt;li>\( w \) is the original word embedding,&lt;/li>
&lt;li>\( b_i \) are bias direction vectors.&lt;/li>
&lt;/ul>
&lt;p>This removes gender/racial correlations while &lt;strong>preserving semantic meaning&lt;/strong>.&lt;/p>
&lt;hr>
&lt;h2 id="3-adversarial-learning-for-bias-and-toxicity-mitigation">&lt;strong>3. Adversarial Learning for Bias and Toxicity Mitigation&lt;/strong>&lt;/h2>
&lt;p>We use &lt;strong>adversarial debiasing&lt;/strong> to remove bias from models while maintaining accuracy.&lt;/p>
&lt;h3 id="31-adversarial-loss-function">&lt;strong>3.1. Adversarial Loss Function&lt;/strong>&lt;/h3>
&lt;p>A language model \( M \) is trained with two competing objectives:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Minimize classification loss&lt;/strong> \( L_C \).&lt;/li>
&lt;li>&lt;strong>Maximize bias confusion loss&lt;/strong> \( L_B \).&lt;/li>
&lt;/ol>
\[
L = L_C(X, y) - \lambda L_B(X, b)
\]&lt;p>where:&lt;/p>
&lt;ul>
&lt;li>\( X \) is the input text,&lt;/li>
&lt;li>\( y \) is the label (e.g., toxic/non-toxic),&lt;/li>
&lt;li>\( b \) is the protected attribute (e.g., gender),&lt;/li>
&lt;li>\( \lambda \) controls trade-off between accuracy and fairness.&lt;/li>
&lt;/ul>
&lt;h3 id="32-differentially-private-training">&lt;strong>3.2. Differentially Private Training&lt;/strong>&lt;/h3>
&lt;p>To prevent models from &lt;strong>memorizing biased patterns&lt;/strong>, we use &lt;strong>differential privacy (DP)&lt;/strong>:&lt;/p>
\[
P(M(X) = y) \approx P(M(X') = y) + \epsilon
\]&lt;p>where:&lt;/p>
&lt;ul>
&lt;li>\( X' \) is a &lt;strong>slightly modified&lt;/strong> version of \( X \),&lt;/li>
&lt;li>\( \epsilon \) is the &lt;strong>privacy budget&lt;/strong> (smaller is better).&lt;/li>
&lt;/ul>
&lt;p>Using &lt;strong>DP-SGD (Differentially Private Stochastic Gradient Descent)&lt;/strong>:&lt;/p>
\[
W_{t+1} = W_t - \eta \left( \nabla L(W_t) + \mathcal{N}(0, \sigma^2) \right)
\]&lt;p>where &lt;strong>Gaussian noise \( \mathcal{N}(0, \sigma^2) \)&lt;/strong> ensures individual examples donâ€™t overly influence model behavior.&lt;/p>
&lt;hr>
&lt;h2 id="4-fair-nlp-generation-and-detoxification">&lt;strong>4. Fair NLP Generation and Detoxification&lt;/strong>&lt;/h2>
&lt;h3 id="41-controlled-text-generation-with-fair-constraints">&lt;strong>4.1. Controlled Text Generation with Fair Constraints&lt;/strong>&lt;/h3>
&lt;p>We modify &lt;strong>text generation objectives&lt;/strong> by adding fairness constraints:&lt;/p>
\[
P(W | C) = \frac{P(C | W) P(W)}{P(C)}
\]&lt;p>where:&lt;/p>
&lt;ul>
&lt;li>\( P(W | C) \) is the probability of generating &lt;strong>word \( W \) given context \( C \)&lt;/strong>.&lt;/li>
&lt;li>&lt;strong>Penalty for unfair text&lt;/strong>:&lt;/li>
&lt;/ul>
\[
L(W) = L_{\text{LM}}(W) + \lambda \sum_{i} P(W | b_i)
\]&lt;p>where \( b_i \) are &lt;strong>biased word categories&lt;/strong>.&lt;/p>
&lt;h3 id="42-reinforcement-learning-for-detoxification">&lt;strong>4.2. Reinforcement Learning for Detoxification&lt;/strong>&lt;/h3>
&lt;p>To &lt;strong>detoxify language models&lt;/strong>, we optimize a &lt;strong>reward function&lt;/strong>:&lt;/p>
\[
R(W) = R_{\text{fluency}}(W) + \alpha R_{\text{fairness}}(W) - \beta R_{\text{toxicity}}(W)
\]&lt;p>where:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>\( R_{\text{fluency}} \)&lt;/strong>: Ensures coherent outputs.&lt;/li>
&lt;li>&lt;strong>\( R_{\text{fairness}} \)&lt;/strong>: Penalizes biased outputs.&lt;/li>
&lt;li>&lt;strong>\( R_{\text{toxicity}} \)&lt;/strong>: Penalizes offensive language.&lt;/li>
&lt;/ul>
&lt;p>Using &lt;strong>PPO (Proximal Policy Optimization)&lt;/strong>:&lt;/p>
\[
\theta_{t+1} = \theta_t + \eta \mathbb{E} \left[ \nabla_{\theta} \log \pi_{\theta} (W) R(W) \right]
\]&lt;p>where:&lt;/p>
&lt;ul>
&lt;li>\( \pi_{\theta} \) is the modelâ€™s policy,&lt;/li>
&lt;li>\( R(W) \) is the fairness-aware reward function.&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="5-real-world-applications-of-nlp-for-fair-ai">&lt;strong>5. Real-World Applications of NLP for Fair AI&lt;/strong>&lt;/h2>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;strong>Use Case&lt;/strong>&lt;/th>
&lt;th>&lt;strong>Method Used&lt;/strong>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;strong>Toxic Comment Detection&lt;/strong>&lt;/td>
&lt;td>Transformer-based classifiers (e.g., BERT, RoBERTa)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;strong>Bias-Free Resume Screening&lt;/strong>&lt;/td>
&lt;td>Adversarial debiasing in NLP models&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;strong>Safe AI Chatbots&lt;/strong>&lt;/td>
&lt;td>Controlled generation using RL-based detoxification&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;strong>Fair Sentiment Analysis&lt;/strong>&lt;/td>
&lt;td>Sentiment classifiers trained with fairness constraints&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;hr>
&lt;!-- This is a comment -->
&lt;!--
# Implementations
## **1. Introduction**
Natural Language Processing (NLP) and Large Language Models (LLMs) can be leveraged to **detect, mitigate, and prevent** toxicity and unconscious bias in text-based interactions. Techniques like **bias detection, sentiment analysis, adversarial training, and fairness-aware modeling** help ensure ethical AI deployment in diverse applications.
---
## **2. Key Challenges**
### **2.1. Toxicity in Language**
- **Hate Speech &amp; Harassment**: Offensive language targeting race, gender, religion, etc.
- **Misinformation &amp; Harmful Content**: Spreading false or harmful narratives.
- **Implicit Toxicity**: Sarcastic, coded, or context-dependent toxicity.
### **2.2. Unconscious Bias in Language**
- **Social Bias**: Stereotypes based on gender, ethnicity, or socioeconomic status.
- **Historical Bias**: AI trained on biased historical data continues patterns of discrimination.
- **Selection Bias**: NLP systems may favor dominant linguistic patterns (e.g., English over other languages).
---
## **3. NLP Techniques for Addressing Toxicity and Bias**
### **3.1. Text Classification for Toxicity Detection**
- **Approach**: Use supervised learning to classify text as **toxic** or **non-toxic**.
- **Example:** Using **BERT**, **GPT**, or **RoBERTa** for classification.
#### **Sample Code:**
```python
from transformers import pipeline
# Load a toxicity classifier model (pre-trained)
classifier = pipeline("text-classification", model="unitary/toxic-bert")
# Test toxic text detection
texts = ["I hate you!", "You are amazing!", "Some people are just so ignorant."]
results = classifier(texts)
# Display results
for text, result in zip(texts, results):
print(f"Text: {text} -- Label: {result['label']} (Score: {result['score']:.2f})")
```
**Enhancements**: Fine-tune models with domain-specific toxic datasets (e.g., **Jigsaw Toxic Comment Dataset**).
---
### **3.2. Bias Detection in Language Models**
- **Approach**: Analyze embeddings and outputs for biased associations.
- **Technique**: **Word Embedding Association Test (WEAT)** measures bias in word associations.
#### **Example: Bias in Word Embeddings**
```python
from whatlies.language import SpacyLanguage
from whatlies.embeddingset import EmbeddingSet
# Load language model
nlp = SpacyLanguage("en_core_web_md")
# Define gender-related words
words = ["doctor", "nurse", "engineer", "teacher", "man", "woman"]
# Get embeddings
embeddings = EmbeddingSet({word: nlp[word] for word in words})
# Visualize bias
embeddings.plot_interactive(x_axis="man", y_axis="woman")
```
- **Interpretation**: If gender-neutral words (e.g., "doctor", "nurse") cluster near gendered terms, the model reflects bias.
- **Mitigation**: **Re-training embeddings** using debiased datasets.
---
### **3.3. Adversarial Training to Reduce Bias**
- **Approach**: Train language models to **minimize correlations** between bias-sensitive features and predictions.
#### **Example: Adversarial Debiasing Model**
```python
from fairlearn.reductions import ExponentiatedGradient
from fairlearn.metrics import demographic_parity_difference
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification
# Generate biased dataset
X, y = make_classification(n_samples=1000, n_features=10)
sensitive_feature = X[:, 0] # Simulated gender bias
# Train a fair model
base_model = LogisticRegression()
fair_model = ExponentiatedGradient(base_model, constraints="demographic_parity")
fair_model.fit(X, y, sensitive_features=sensitive_feature)
# Evaluate bias reduction
print("Demographic Parity Difference:", demographic_parity_difference(y, fair_model.predict(X), sensitive_feature))
```
- **Benefit**: Reduces the impact of **biased correlations** in training data.
---
### **3.4. Sentiment Analysis for Implicit Toxicity**
- **Approach**: Use sentiment classification with additional **toxicity** labels.
- **Example:** Classify text into **positive, negative, or toxic**.
#### **Sample Code:**
```python
from transformers import pipeline
# Load sentiment analysis model
sentiment_pipeline = pipeline("sentiment-analysis")
# Test with toxic language
texts = ["I love this place!", "You are so dumb!", "The service was terrible."]
results = sentiment_pipeline(texts)
# Display results
for text, result in zip(texts, results):
print(f"Text: {text} -- Sentiment: {result['label']} (Score: {result['score']:.2f})")
```
- **Enhancements**: Train on **toxicity-aware sentiment datasets**.
---
### **3.5. Fairness-Aware Text Generation**
- **Approach**: Modify **text generation objectives** to encourage fairness.
- **Technique**: Penalize biased outputs during training.
#### **Example: Controlled Text Generation**
```python
from transformers import pipeline
# Load GPT model with safe prompts
generator = pipeline("text-generation", model="gpt2")
# Generate unbiased responses
prompt = "A successful engineer is often"
output = generator(prompt, max_length=20, num_return_sequences=3)
# Display results
for i, text in enumerate(output):
print(f"Generated Text {i+1}: {text['generated_text']}")
```
- **Enhancement**: Use **prefix-tuning** to steer models towards neutral, inclusive language.
---
## **4. Real-World Applications**
### **4.1. Social Media Content Moderation**
- **Use Case**: Detect and filter hate speech in social networks.
- **Solution**: Integrate **real-time toxicity classification** in comment filtering.
### **4.2. Fair AI Recruitment**
- **Use Case**: Prevent **gender/racial bias** in job applicant screening.
- **Solution**: Train **fair NLP models** to score applicants **without biased attributes**.
### **4.3. Inclusive Chatbots &amp; Virtual Assistants**
- **Use Case**: Ensure **AI-generated responses** avoid stereotypes.
- **Solution**: Fine-tune **dialog models** with **bias-aware training datasets**.
---
## **5. Conclusion**
Using NLP and Language Models, we can:
1. **Detect toxicity** (Toxic BERT, GPT).
2. **Identify unconscious bias** (WEAT, Fairlearn).
3. **Reduce bias through adversarial training**.
4. **Generate fair and inclusive text**.
These techniques **promote ethical AI**, ensuring that language models reflect **diverse, fair, and safe** perspectives. ðŸš€
--></description></item><item><title>How large language and deep learning models can prevent toxicity such as unconscious biases</title><link>https://foundjem.github.io/event/openinfra23/</link><pubDate>Thu, 01 Jun 2023 13:00:00 +0000</pubDate><guid>https://foundjem.github.io/event/openinfra23/</guid><description>&lt;script type="text/javascript" src= '/js/pdf-js/build/pdf.js'>&lt;/script>
&lt;style>
#embed-pdf-container {
position: relative;
width: 100%;
height: auto;
min-height: 20vh;
}
.pdf-canvas {
border: 1px solid black;
direction: ltr;
width: 100%;
height: auto;
display: none;
}
#the-canvas {
border: 1px solid black;
direction: ltr;
width: 100%;
height: auto;
display: none;
}
.pdf-loadingWrapper {
display: none;
justify-content: center;
align-items: center;
width: 100%;
height: 350px;
}
.pdf-loading {
display: inline-block;
width: 50px;
height: 50px;
border: 3px solid #d2d0d0;;
border-radius: 50%;
border-top-color: #383838;
animation: spin 1s ease-in-out infinite;
-webkit-animation: spin 1s ease-in-out infinite;
}
#overlayText {
word-wrap: break-word;
display: grid;
justify-content: end;
}
#overlayText a {
position: relative;
top: 10px;
right: 4px;
color: #000;
margin: auto;
background-color: #eeeeee;
padding: 0.3em 1em;
border: solid 2px;
border-radius: 12px;
border-color: #00000030;
text-decoration: none;
}
#overlayText svg {
height: clamp(1em, 2vw, 1.4em);
width: clamp(1em, 2vw, 1.4em);
}
@keyframes spin {
to { -webkit-transform: rotate(360deg); }
}
@-webkit-keyframes spin {
to { -webkit-transform: rotate(360deg); }
}
&lt;/style>&lt;div class="embed-pdf-container" id="embed-pdf-container-c5e6d382">
&lt;div class="pdf-loadingWrapper" id="pdf-loadingWrapper-c5e6d382">
&lt;div class="pdf-loading" id="pdf-loading-c5e6d382">&lt;/div>
&lt;/div>
&lt;div id="overlayText">
&lt;a href="./OpenInfra_Speaker.pdf" aria-label="Download" download>
&lt;svg aria-hidden="true" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 18 18">
&lt;path d="M9 13c.3 0 .5-.1.7-.3L15.4 7 14 5.6l-4 4V1H8v8.6l-4-4L2.6 7l5.7 5.7c.2.2.4.3.7.3zm-7 2h14v2H2z" />
&lt;/svg>
&lt;/a>
&lt;/div>
&lt;canvas class="pdf-canvas" id="pdf-canvas-c5e6d382">&lt;/canvas>
&lt;/div>
&lt;div class="pdf-paginator" id="pdf-paginator-c5e6d382">
&lt;button id="pdf-prev-c5e6d382">Previous&lt;/button>
&lt;button id="pdf-next-c5e6d382">Next&lt;/button> &amp;nbsp; &amp;nbsp;
&lt;span>
&lt;span class="pdf-pagenum" id="pdf-pagenum-c5e6d382">&lt;/span> / &lt;span class="pdf-pagecount" id="pdf-pagecount-c5e6d382">&lt;/span>
&lt;/span>
&lt;a class="pdf-source" id="pdf-source-c5e6d382" href="./OpenInfra_Speaker.pdf">[pdf]&lt;/a>
&lt;/div>
&lt;noscript>
View the PDF file &lt;a class="pdf-source" id="pdf-source-noscript-c5e6d382" href="./OpenInfra_Speaker.pdf">here&lt;/a>.
&lt;/noscript>
&lt;script type="text/javascript">
(function(){
var url = '.\/OpenInfra_Speaker.pdf';
var hidePaginator = "" === "true";
var hideLoader = "" === "true";
var selectedPageNum = parseInt("") || 1;
var pdfjsLib = window['pdfjs-dist/build/pdf'];
if (pdfjsLib.GlobalWorkerOptions.workerSrc == '')
pdfjsLib.GlobalWorkerOptions.workerSrc = "https:\/\/foundjem.github.io\/" + 'js/pdf-js/build/pdf.worker.js';
var pdfDoc = null,
pageNum = selectedPageNum,
pageRendering = false,
pageNumPending = null,
scale = 3,
canvas = document.getElementById('pdf-canvas-c5e6d382'),
ctx = canvas.getContext('2d'),
paginator = document.getElementById("pdf-paginator-c5e6d382"),
loadingWrapper = document.getElementById('pdf-loadingWrapper-c5e6d382');
showPaginator();
showLoader();
function renderPage(num) {
pageRendering = true;
pdfDoc.getPage(num).then(function(page) {
var viewport = page.getViewport({scale: scale});
canvas.height = viewport.height;
canvas.width = viewport.width;
var renderContext = {
canvasContext: ctx,
viewport: viewport
};
var renderTask = page.render(renderContext);
renderTask.promise.then(function() {
pageRendering = false;
showContent();
if (pageNumPending !== null) {
renderPage(pageNumPending);
pageNumPending = null;
}
});
});
document.getElementById('pdf-pagenum-c5e6d382').textContent = num;
}
function showContent() {
loadingWrapper.style.display = 'none';
canvas.style.display = 'block';
}
function showLoader() {
if(hideLoader) return
loadingWrapper.style.display = 'flex';
canvas.style.display = 'none';
}
function showPaginator() {
if(hidePaginator) return
paginator.style.display = 'block';
}
function queueRenderPage(num) {
if (pageRendering) {
pageNumPending = num;
} else {
renderPage(num);
}
}
function onPrevPage() {
if (pageNum &lt;= 1) {
return;
}
pageNum--;
queueRenderPage(pageNum);
}
document.getElementById('pdf-prev-c5e6d382').addEventListener('click', onPrevPage);
function onNextPage() {
if (pageNum >= pdfDoc.numPages) {
return;
}
pageNum++;
queueRenderPage(pageNum);
}
document.getElementById('pdf-next-c5e6d382').addEventListener('click', onNextPage);
pdfjsLib.getDocument(url).promise.then(function(pdfDoc_) {
pdfDoc = pdfDoc_;
var numPages = pdfDoc.numPages;
document.getElementById('pdf-pagecount-c5e6d382').textContent = numPages;
if(pageNum > numPages) {
pageNum = numPages
}
renderPage(pageNum);
});
})();
&lt;/script>
&lt;div class="flex px-4 py-3 mb-6 rounded-md bg-primary-100 dark:bg-primary-900">
&lt;span class="pr-3 pt-1 text-primary-600 dark:text-primary-300">
&lt;svg height="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">&lt;path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z"/>&lt;/svg>
&lt;/span>
&lt;span class="dark:text-neutral-300">Click on the &lt;strong>Download&lt;/strong> button above to view the built-in slides feature.&lt;/span>
&lt;/div></description></item></channel></rss>